{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 — Orientation & Setup\n",
        "\n",
        "Goal: Ensure environment and data access are ready, and document key columns.\n",
        "\n",
        "Checklist\n",
        "- [ ] Read the project Task Prompt and objectives\n",
        "- [ ] Confirm Python/Git/IDE installed and accessible\n",
        "- [ ] Verify access to CRMLS CSV datasets in this folder\n",
        "- [ ] Deliverable: Short notes describing key columns\n",
        "\n",
        "Instructions\n",
        "- Run the cells in this Week 1 section first.\n",
        "- If any check fails, add notes here and we’ll fix it before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'python': '3.12.4', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'git': True}\n",
            "{'jupyter': True}\n"
          ]
        }
      ],
      "source": [
        "# Week 1: Environment check\n",
        "import sys, subprocess\n",
        "import shutil\n",
        "import platform\n",
        "\n",
        "print({\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"platform\": platform.platform(),\n",
        "    \"git\": shutil.which(\"git\") is not None,\n",
        "})\n",
        "\n",
        "# IDE check (heuristic): running in Jupyter implies OK\n",
        "print({\"jupyter\": True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10 CSV files\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PosixPath('/Users/anvith/Downloads/Datasets IDX/CRMLSSold202501_filled.csv'),\n",
              " PosixPath('/Users/anvith/Downloads/Datasets IDX/CRMLSSold202502_filled.csv'),\n",
              " PosixPath('/Users/anvith/Downloads/Datasets IDX/CRMLSSold202503_filled.csv'),\n",
              " PosixPath('/Users/anvith/Downloads/Datasets IDX/CRMLSSold202504_filled.csv'),\n",
              " PosixPath('/Users/anvith/Downloads/Datasets IDX/CRMLSSold202505_filled.csv')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Week 1: Dataset access confirmation\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "DATA_DIR = Path.cwd()\n",
        "RAW_PATTERN = re.compile(r\"^CRMLSSold\\d{6}.*\\.csv$\", re.I)\n",
        "raw_files = sorted([p for p in DATA_DIR.iterdir() if p.is_file() and RAW_PATTERN.match(p.name)])\n",
        "\n",
        "print(f\"Found {len(raw_files)} CSV files\")\n",
        "raw_files[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample file: CRMLSSold202501_filled.csv\n",
            "Columns: 80\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Index(['BuyerAgentAOR', 'ListAgentAOR', 'Flooring', 'ViewYN', 'WaterfrontYN',\n",
              "       'BasementYN', 'PoolPrivateYN', 'OriginalListPrice', 'ListingKey',\n",
              "       'ListAgentEmail', 'CloseDate', 'ClosePrice', 'ListAgentFirstName',\n",
              "       'ListAgentLastName', 'Latitude', 'Longitude', 'UnparsedAddress',\n",
              "       'PropertyType', 'LivingArea', 'ListPrice', 'DaysOnMarket',\n",
              "       'ListOfficeName', 'BuyerOfficeName', 'CoListOfficeName',\n",
              "       'ListAgentFullName', 'CoListAgentFirstName', 'CoListAgentLastName',\n",
              "       'BuyerAgentMlsId', 'BuyerAgentFirstName', 'BuyerAgentLastName',\n",
              "       'FireplacesTotal', 'AssociationFeeFrequency', 'AboveGradeFinishedArea',\n",
              "       'ListingKeyNumeric', 'MLSAreaMajor', 'TaxAnnualAmount',\n",
              "       'CountyOrParish', 'MlsStatus', 'ElementarySchool', 'AttachedGarageYN',\n",
              "       'ParkingTotal', 'BuilderName', 'PropertySubType', 'LotSizeAcres',\n",
              "       'SubdivisionName', 'BuyerOfficeAOR', 'YearBuilt', 'StreetNumberNumeric',\n",
              "       'ListingId', 'BathroomsTotalInteger'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Peek first file for columns\n",
        "import pandas as pd\n",
        "\n",
        "if not raw_files:\n",
        "    raise SystemExit(\"No CRMLS CSVs found. Place files like CRMLSSoldYYYYMM_*.csv in this folder.\")\n",
        "\n",
        "sample_path = raw_files[0]\n",
        "df_sample = pd.read_csv(sample_path, nrows=200, dtype=str, low_memory=False)\n",
        "df_sample.columns = [c.strip() for c in df_sample.columns]\n",
        "print(\"Sample file:\", sample_path.name)\n",
        "print(\"Columns:\", len(df_sample.columns))\n",
        "df_sample.columns[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matched columns (notes):\n",
            "- ClosePrice: Final sales price at closing; modeling target.\n",
            "- CloseDate: Date the sale closed; used for time-aware splits and trend features.\n",
            "- LivingArea: Interior living area (sq ft). Used for PPSF and size.\n",
            "- BedroomsTotal: Number of bedrooms.\n",
            "- BathroomsTotalInteger: Number of bathrooms (integer).\n",
            "- LotSizeAcres: Lot size in acres.\n",
            "- YearBuilt: Year the property was built.\n",
            "- GarageSpaces: Number of garage spaces.\n",
            "- PoolPrivateYN: Whether the property has a private pool.\n",
            "- Latitude: Latitude coordinate for geo features.\n",
            "- Longitude: Longitude coordinate for geo features.\n",
            "- PostalCode: ZIP/postal code for neighborhood aggregation.\n",
            "- City: City name.\n",
            "- CountyOrParish: County for geographic grouping.\n",
            "- ElementarySchool: Assigned elementary school/district indicator.\n"
          ]
        }
      ],
      "source": [
        "# Week 1 Deliverable: Short notes for key columns\n",
        "\n",
        "# Map likely important columns to human-friendly descriptions.\n",
        "# Adjust as needed if your schema differs.\n",
        "key_columns = {\n",
        "    \"ClosePrice\": \"Final sales price at closing; modeling target.\",\n",
        "    \"CloseDate\": \"Date the sale closed; used for time-aware splits and trend features.\",\n",
        "    \"LivingArea\": \"Interior living area (sq ft). Used for PPSF and size.\",\n",
        "    \"BedroomsTotal\": \"Number of bedrooms.\",\n",
        "    \"BathroomsTotalInteger\": \"Number of bathrooms (integer).\",\n",
        "    \"LotSizeAcres\": \"Lot size in acres.\",\n",
        "    \"YearBuilt\": \"Year the property was built.\",\n",
        "    \"GarageSpaces\": \"Number of garage spaces.\",\n",
        "    \"PoolPrivateYN\": \"Whether the property has a private pool.\",\n",
        "    \"Latitude\": \"Latitude coordinate for geo features.\",\n",
        "    \"Longitude\": \"Longitude coordinate for geo features.\",\n",
        "    \"PostalCode\": \"ZIP/postal code for neighborhood aggregation.\",\n",
        "    \"City\": \"City name.\",\n",
        "    \"CountyOrParish\": \"County for geographic grouping.\",\n",
        "    \"ElementarySchool\": \"Assigned elementary school/district indicator.\",\n",
        "}\n",
        "\n",
        "# Try to match with actual columns (case-insensitive substring contains)\n",
        "actual_cols = list(df_sample.columns)\n",
        "matched = {}\n",
        "unmatched = []\n",
        "for k, note in key_columns.items():\n",
        "    m = next((c for c in actual_cols if k.lower() == c.lower() or k.lower() in c.lower()), None)\n",
        "    if m:\n",
        "        matched[m] = note\n",
        "    else:\n",
        "        unmatched.append(k)\n",
        "\n",
        "print(\"Matched columns (notes):\")\n",
        "for c, note in matched.items():\n",
        "    print(f\"- {c}: {note}\")\n",
        "\n",
        "if unmatched:\n",
        "    print(\"\\nColumns not found (review and adjust names if needed):\")\n",
        "    for k in unmatched:\n",
        "        print(f\"- {k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If any of the above key columns are missing or named differently, please provide your schema or an example row so I can refine the mapping precisely (e.g., `ClosePrice` is `ClosePrice` vs `closeprice`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CRMLS Close Price Prediction\n",
        "\n",
        "This notebook builds an end-to-end, leakage-safe model to predict property close price from CRMLS historical data.\n",
        "\n",
        "- Loads only the original CSVs in this folder\n",
        "- Cleans and combines monthly files\n",
        "- Excludes listing-only fields (ListPrice, DaysOnMarket, agents, marketing) to avoid leakage\n",
        "- Engineers neighborhood-level signals (median price per sqft using only prior sales)\n",
        "- Trains and evaluates a baseline gradient boosting model\n",
        "- Provides a lightweight prediction function for new properties\n",
        "\n",
        "Notes:\n",
        "- All work is performed in-notebook; no external artifacts are written.\n",
        "- Replace the sample prediction with a real property when available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import json\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "DATA_DIR = Path.cwd()\n",
        "RAW_PATTERN = re.compile(r\"^CRMLSSold\\d{6}.*\\.csv$\", re.I)\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "np.set_printoptions(suppress=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discover and load raw CSVs\n",
        "raw_files = sorted([p for p in DATA_DIR.iterdir() if p.is_file() and RAW_PATTERN.match(p.name)])\n",
        "if not raw_files:\n",
        "    raise SystemExit(\"No CRMLS CSV files found in the current directory.\")\n",
        "\n",
        "raw_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standardize_column_names(columns: List[str]) -> List[str]:\n",
        "    out = []\n",
        "    seen = {}\n",
        "    for col in columns:\n",
        "        c = col.strip()\n",
        "        c = re.sub(r\"\\s+\", \"_\", c)\n",
        "        c = c.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
        "        c = c.replace(\"(\", \"\").replace(\")\", \"\")\n",
        "        c = c.replace(\"%\", \"pct\").replace(\"#\", \"num\").replace(\"$\", \"usd\")\n",
        "        c = c.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
        "        c = re.sub(r\"[^0-9A-Za-z_]+\", \"\", c)\n",
        "        c = re.sub(r\"_+\", \"_\", c).strip(\"_\").lower()\n",
        "        if not c:\n",
        "            c = \"col\"\n",
        "        if c in seen:\n",
        "            seen[c] += 1\n",
        "            c = f\"{c}_{seen[c]}\"\n",
        "        else:\n",
        "            seen[c] = 0\n",
        "        out.append(c)\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_and_clean(path: Path) -> pd.DataFrame:\n",
        "    # read as strings first; let later steps cast\n",
        "    df = pd.read_csv(path, dtype=str, low_memory=False)\n",
        "    df.columns = standardize_column_names(list(df.columns))\n",
        "    # drop repeated header rows\n",
        "    header_tuple = tuple(df.columns)\n",
        "    mask = df.apply(lambda r: tuple(r.astype(str)) == header_tuple, axis=1)\n",
        "    if mask.any():\n",
        "        df = df.loc[~mask].copy()\n",
        "    # trim strings and set common NAs\n",
        "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
        "    for c in obj_cols:\n",
        "        s = df[c].astype(\"string\").str.strip()\n",
        "        df[c] = s.replace({\"\": pd.NA, \"na\": pd.NA, \"n/a\": pd.NA, \"none\": pd.NA})\n",
        "    return df\n",
        "\n",
        "\n",
        "dfs = [load_and_clean(p) for p in raw_files]\n",
        "len(dfs), [df.shape for df in dfs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine and basic info\n",
        "combined = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "combined = combined.drop_duplicates()\n",
        "print(combined.shape)\n",
        "combined.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect target and date columns\n",
        "TARGET_CANDS = [\"close_price\",\"closeprice\",\"sold_price\",\"soldprice\",\"sale_price\",\"saleprice\",\"final_sales_price\",\"finalsalesprice\",\"final_price\",\"finalprice\",\"closing_price\",\"closingprice\"]\n",
        "DATE_CANDS = [\"close_date\",\"closedate\",\"sold_date\",\"solddate\",\"closing_date\",\"closingdate\",\"recording_date\"]\n",
        "\n",
        "cols_lower = {c.lower(): c for c in combined.columns}\n",
        "\n",
        "def detect_col(cands):\n",
        "    for c in cands:\n",
        "        if c in cols_lower:\n",
        "            return cols_lower[c]\n",
        "    for c in cands:\n",
        "        for col in combined.columns:\n",
        "            if c in col.lower():\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "TARGET = detect_col(TARGET_CANDS)\n",
        "DATE_COL = detect_col(DATE_CANDS)\n",
        "TARGET, DATE_COL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove leakage-prone columns and select property/location features\n",
        "LEAKAGE_KEYS = [\"listprice\",\"originallistprice\",\"original_list_price\",\"list_price\",\"listing\",\"dom\",\"days_on_market\",\"daysonmarket\",\"cdom\",\"status\",\"agent\",\"broker\",\"office\",\"remarks\",\"marketing\",\"virtual\",\"photo\",\"syndication\",\"license\",\"member\",\"mls\"]\n",
        "PROP_KEYS = [\"bed\",\"bath\",\"living_area\",\"sqft\",\"square_feet\",\"lot\",\"lot_size\",\"acres\",\"year_built\",\"yearbuilt\",\"stories\",\"units\",\"pool\",\"spa\",\"garage\",\"parking\",\"hoa\",\"zoning\"]\n",
        "LOC_KEYS  = [\"latitude\",\"longitude\",\"lat\",\"lon\",\"lng\",\"zip\",\"zipcode\",\"postal\",\"postalcode\",\"city\",\"county\",\"neighborhood\",\"school\",\"district\"]\n",
        "\n",
        "feature_cols = []\n",
        "for c in combined.columns:\n",
        "    if c == TARGET: continue\n",
        "    low = c.lower()\n",
        "    if any(k in low for k in LEAKAGE_KEYS):\n",
        "        continue\n",
        "    if any(k in low for k in PROP_KEYS) or any(k in low for k in LOC_KEYS) or (DATE_COL and c == DATE_COL):\n",
        "        feature_cols.append(c)\n",
        "\n",
        "len(feature_cols), feature_cols[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cast types and engineer neighborhood PPSF features\n",
        "\n",
        "def to_numeric_clean(s: pd.Series) -> pd.Series:\n",
        "    if s.dtype.kind in \"biufc\":\n",
        "        return s\n",
        "    x = s.astype(str)\n",
        "    x = x.str.replace(\",\", \"\", regex=False).str.replace(\"$\", \"\", regex=False).str.replace(\"%\", \"\", regex=False)\n",
        "    return pd.to_numeric(x, errors=\"coerce\")\n",
        "\n",
        "work = combined[feature_cols + ([TARGET] if TARGET else [])].copy()\n",
        "\n",
        "# target numeric\n",
        "if TARGET is None:\n",
        "    raise SystemExit(\"Could not detect target close price column.\")\n",
        "work[TARGET] = to_numeric_clean(work[TARGET])\n",
        "\n",
        "# likely area column\n",
        "AREA_CANDS = [\"living_area\",\"livingarea\",\"sqft\",\"square_feet\",\"square_footage\"]\n",
        "area_col = next((c for c in AREA_CANDS if c in work.columns), None)\n",
        "if area_col is not None:\n",
        "    area = to_numeric_clean(work[area_col])\n",
        "    work[\"ppsf_target\"] = work[TARGET] / area\n",
        "\n",
        "# date column\n",
        "if DATE_COL and DATE_COL in work.columns:\n",
        "    work[DATE_COL] = pd.to_datetime(work[DATE_COL], errors=\"coerce\")\n",
        "\n",
        "# zip/postal\n",
        "zip_col = next((c for c in [\"zip\",\"zipcode\",\"postal\",\"postalcode\"] if c in work.columns), None)\n",
        "\n",
        "# rolling by count (prior N sales)\n",
        "if zip_col is not None and \"ppsf_target\" in work.columns:\n",
        "    tmp = work[[zip_col, DATE_COL] if DATE_COL else [zip_col]].copy()\n",
        "    if DATE_COL:\n",
        "        tmp = tmp.join(work[[\"ppsf_target\"]])\n",
        "        tmp = tmp.sort_values([zip_col, DATE_COL])\n",
        "    else:\n",
        "        tmp = tmp.join(work[[\"ppsf_target\"]])\n",
        "        tmp = tmp.sort_values([zip_col]).reset_index(drop=True)\n",
        "    def grp_roll(g):\n",
        "        return g[\"ppsf_target\"].rolling(window=200, min_periods=10).median().shift(1)\n",
        "    tmp[\"zip_median_ppsf_prev\"] = tmp.groupby(zip_col, group_keys=False).apply(grp_roll)\n",
        "    work[\"zip_median_ppsf_prev\"] = tmp[\"zip_median_ppsf_prev\"].values\n",
        "\n",
        "# time-window (90D) prior median if dates available\n",
        "if zip_col is not None and DATE_COL and \"ppsf_target\" in work.columns:\n",
        "    g = work[[zip_col, DATE_COL, \"ppsf_target\"]].dropna(subset=[DATE_COL]).copy()\n",
        "    g = g.sort_values([zip_col, DATE_COL])\n",
        "    def time_roll(h):\n",
        "        s = h.set_index(DATE_COL)[\"ppsf_target\"]\n",
        "        return s.rolling(\"90D\", min_periods=10).median().shift(1)\n",
        "    rolled = g.groupby(zip_col, group_keys=False).apply(time_roll).rename(\"zip_median_ppsf_prev_90d\").reset_index()\n",
        "    work = work.merge(rolled, how=\"left\", on=[zip_col, DATE_COL])\n",
        "\n",
        "work.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset for modeling\n",
        "# Drop rows without target\n",
        "model_df = work.dropna(subset=[TARGET]).copy()\n",
        "\n",
        "X = model_df.drop(columns=[TARGET])\n",
        "y = pd.to_numeric(model_df[TARGET], errors=\"coerce\")\n",
        "mask = y.notna()\n",
        "X = X.loc[mask]\n",
        "y = y.loc[mask]\n",
        "\n",
        "# Identify numeric vs categorical\n",
        "num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
        "cat_cols = [c for c in X.columns if c not in num_cols]\n",
        "\n",
        "len(X), len(num_cols), len(cat_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time-aware split if possible\n",
        "if DATE_COL and DATE_COL in X.columns:\n",
        "    dates = pd.to_datetime(X[DATE_COL], errors=\"coerce\")\n",
        "    order = dates.argsort(kind=\"mergesort\")\n",
        "    X_sorted = X.iloc[order]\n",
        "    y_sorted = y.iloc[order]\n",
        "    split_idx = int(0.8 * len(X_sorted))\n",
        "    X_train, X_test = X_sorted.iloc[:split_idx], X_sorted.iloc[split_idx:]\n",
        "    y_train, y_test = y_sorted.iloc[:split_idx], y_sorted.iloc[split_idx:]\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build pipeline\n",
        "numeric_transformer = SimpleImputer(strategy=\"median\")\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=20)),\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "      (\"num\", numeric_transformer, [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]),\n",
        "      (\"cat\", categorical_transformer, [c for c in X_train.columns if not pd.api.types.is_numeric_dtype(X_train[c])]),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3,\n",
        ")\n",
        "\n",
        "model = HistGradientBoostingRegressor(\n",
        "    max_depth=None,\n",
        "    learning_rate=0.08,\n",
        "    max_iter=600,\n",
        "    min_samples_leaf=25,\n",
        "    l2_regularization=0.05,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", model)])\n",
        "pipe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "y_pred = pipe.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mape = float(np.mean(np.abs((y_test - y_pred) / np.clip(np.abs(y_test), 1e-9, None))))\n",
        "rmse = float(np.sqrt(np.mean((y_test - y_pred) ** 2)))\n",
        "\n",
        "print({\"R2\": r2, \"MAE\": mae, \"MAPE\": mape, \"RMSE\": rmse})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick EDA snippets\n",
        "\n",
        "Use these as needed to inspect the data shape, missingness, and distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA cells (optional to run)\n",
        "combined.info()\n",
        "\n",
        "# Missingness top 20\n",
        "(combined.isna().sum().sort_values(ascending=False).head(20))\n",
        "\n",
        "# Numeric summary\n",
        "combined.select_dtypes(include=[\"number\"]).describe().T.head(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction helper\n",
        "\n",
        "Provide a property dictionary and get a predicted close price. Only property and location features are used; listing-only fields are ignored.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_close_price(pipe, feature_cols: List[str], sample: Dict) -> float:\n",
        "    X = pd.DataFrame([sample])\n",
        "    for c in feature_cols:\n",
        "        if c not in X.columns:\n",
        "            X[c] = np.nan\n",
        "    X = X[feature_cols]\n",
        "    return float(pipe.predict(X)[0])\n",
        "\n",
        "# Example usage (edit values as appropriate):\n",
        "sample = {\n",
        "    \"beds\": 3,\n",
        "    \"bathroomstotalinteger\": 2,\n",
        "    \"livingarea\": 1500,\n",
        "    \"lotsizeacres\": 0.12,\n",
        "    \"zipcode\": \"92880\",\n",
        "    \"yearbuilt\": 1985,\n",
        "    \"latitude\": 33.9,\n",
        "    \"longitude\": -117.6,\n",
        "}\n",
        "\n",
        "try:\n",
        "    pred = predict_close_price(pipe, list(X_train.columns), sample)\n",
        "    print(f\"Predicted close price: ${pred:,.0f}\")\n",
        "except Exception as e:\n",
        "    print(\"Prediction failed; adjust feature names/values:\", e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
